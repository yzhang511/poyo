{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from one.api import ONE\n",
    "path_root = '/home/yzhang39/IBL_foundation_model/'\n",
    "sys.path.append(str(path_root))\n",
    "from src.utils.ibl_data_utils import (\n",
    "    prepare_data,\n",
    "    select_brain_regions, \n",
    "    list_brain_regions, \n",
    "    bin_spiking_data,\n",
    "    load_anytime_behaviors\n",
    ")\n",
    "from kirby.data import Data, IrregularTimeSeries, Interval, DatasetBuilder, ArrayDict\n",
    "\n",
    "from datetime import datetime\n",
    "from kirby.taxonomy import (\n",
    "    Task,\n",
    "    Sex,\n",
    "    Species,\n",
    "    SubjectDescription,\n",
    ")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = ONE(\n",
    "    base_url='https://openalyx.internationalbrainlab.org', \n",
    "    password='international', mode='remote'\n",
    ")\n",
    "\n",
    "freeze_file = f'{path_root}/data/2023_12_bwm_release.csv'\n",
    "bwm_df = pd.read_csv(freeze_file, index_col=0)\n",
    "\n",
    "eid = 'db4df448-e449-4a6f-a0e7-288711e7a75a'\n",
    "\n",
    "params = {\n",
    "    'interval_len': 2, 'binsize': 0.02, 'single_region': False, \n",
    "    'align_time': 'stimOn_times', 'time_window': (-.5, 1.5), 'fr_thresh': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1 probes for session eid: db4df448-e449-4a6f-a0e7-288711e7a75a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use spikes from brain regions:  ['CA1' 'DG' 'LP' 'PoT' 'SGN' 'SNc' 'SPF' 'VISp' 'ZI' 'root']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402/402 [00:03<00:00, 119.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# CAUTION: Match trial selection criteria\n",
    "neural_dict, _, meta_data, trials_data = prepare_data(one, eid, bwm_df, params, n_workers=1)\n",
    "regions, beryl_reg = list_brain_regions(neural_dict, **params)\n",
    "region_cluster_ids = select_brain_regions(neural_dict, beryl_reg, regions, **params)\n",
    "binned_spikes, clusters_used_in_bins = bin_spiking_data(\n",
    "    region_cluster_ids, neural_dict, trials_df=trials_data['trials_df'], n_workers=1, **params\n",
    ")\n",
    "avg_fr = binned_spikes.sum(1).mean(0) / params['interval_len']\n",
    "active_neuron_ids = np.argwhere(avg_fr > 1/params['fr_thresh']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract spiking activity\n",
    "spike_times = neural_dict['spike_times']\n",
    "spike_clusters = neural_dict['spike_clusters']\n",
    "unit_mask = np.isin(spike_clusters, active_neuron_ids)\n",
    "spike_times = spike_times[unit_mask]\n",
    "spike_clusters = spike_clusters[unit_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_ids = np.array(meta_data['uuids'])[active_neuron_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_meta = []\n",
    "timestamps = []\n",
    "unit_index = []\n",
    "\n",
    "for i in range(len(unit_ids)):\n",
    "\n",
    "    unit_id = unit_ids[i]\n",
    "    \n",
    "    # extract spikes\n",
    "    times = spike_times[spike_clusters == i]\n",
    "    timestamps.append(times)\n",
    "\n",
    "    if len(times) > 0:\n",
    "        unit_index.append([i] * len(times))\n",
    "\n",
    "    # extract unit metadata\n",
    "    unit_meta.append(\n",
    "        {\n",
    "            \"id\": unit_id,\n",
    "            \"unit_number\": i,\n",
    "            \"count\": len(times),\n",
    "            \"type\": 0,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "unit_meta_df = pd.DataFrame(unit_meta)  # list of dicts to dataframe\n",
    "units = ArrayDict.from_dataframe(\n",
    "    unit_meta_df,\n",
    "    unsigned_to_long=True,\n",
    ")\n",
    "\n",
    "# concatenate spikes\n",
    "timestamps = np.concatenate(timestamps)\n",
    "unit_index = np.concatenate(unit_index)\n",
    "\n",
    "# create spikes object\n",
    "spikes = IrregularTimeSeries(\n",
    "    timestamps=timestamps,\n",
    "    unit_index=unit_index,\n",
    "    domain=\"auto\",\n",
    ")\n",
    "\n",
    "# make sure to sort ethe spikes\n",
    "spikes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_trials\n",
    "# CAUTION: Need to exclude trials in which behavior is NONE\n",
    "\n",
    "trial_mask = trials_data['trials_mask']\n",
    "start_time = (trials_data['trials_df'][params['align_time']] - params['time_window'][0])[trial_mask]\n",
    "end_time = (trials_data['trials_df'][params['align_time']] + params['time_window'][1])[trial_mask]\n",
    "\n",
    "max_num_trials = sum(trial_mask)\n",
    "trial_idxs = np.random.choice(np.arange(max_num_trials), max_num_trials, replace=False)\n",
    "train_idxs = trial_idxs[:int(0.7*max_num_trials)]\n",
    "val_idxs = trial_idxs[int(0.7*max_num_trials):int(0.8*max_num_trials)]\n",
    "test_idxs = trial_idxs[int(0.8*max_num_trials):]\n",
    "trial_split = np.array(['train'] * max_num_trials)\n",
    "trial_split[val_idxs] = 'val'\n",
    "trial_split[test_idxs] = 'test'\n",
    "\n",
    "trial_table = pd.DataFrame({\n",
    "    \"start\": start_time,\n",
    "    \"end\": end_time,\n",
    "    \"split_indicator\": trial_split,\n",
    "})\n",
    "trials = Interval.from_dataframe(trial_table)\n",
    "\n",
    "train_mask_nwb = trial_table.split_indicator.to_numpy() == \"train\"\n",
    "val_mask_nwb = trial_table.split_indicator.to_numpy() == \"val\"\n",
    "test_mask_nwb = trial_table.split_indicator.to_numpy() == \"test\"\n",
    "\n",
    "trials.train_mask_nwb = (\n",
    "    train_mask_nwb  # Naming with \"_\" since train_mask is reserved\n",
    ")\n",
    "trials.val_mask_nwb = val_mask_nwb\n",
    "trials.test_mask_nwb = test_mask_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract_behavior\n",
    "behave_dict = load_anytime_behaviors(one, eid, n_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = behave_dict['right-whisker-motion-energy']['times']\n",
    "whisker = behave_dict['right-whisker-motion-energy']['values'] / 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_type = np.ones_like(timestamps, dtype=np.int64) * 0\n",
    "\n",
    "# report accuracy only on the evaluation intervals\n",
    "eval_mask = np.zeros_like(timestamps, dtype=bool)\n",
    "\n",
    "for i in range(len(trials)):\n",
    "    eval_mask[\n",
    "        (timestamps >= trials.start[i]) & (timestamps < trials.end[i])\n",
    "    ] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = IrregularTimeSeries(\n",
    "    timestamps=timestamps,\n",
    "    whisker=whisker.reshape(-1,1),\n",
    "    subtask_index=behavior_type,\n",
    "    eval_mask=eval_mask,\n",
    "    domain=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(\n",
    "    # neural activity\n",
    "    spikes=spikes,\n",
    "    units=units,\n",
    "    # stimuli and behavior\n",
    "    trials=trials,\n",
    "    behavior=behavior,\n",
    "    # domain\n",
    "    domain=Interval(trials.start[0], trials.end[-1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3761212,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3761212,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37879"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = {\n",
    "    'data': {\n",
    "        'spikes': ,\n",
    "        'behavior': whisker,\n",
    "    },\n",
    "    'intervals': ,\n",
    "    'train_intervals':,\n",
    "    'test_intervals':\n",
    "    'finetune_intervals': , \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DatasetBuilder(\n",
    "    raw_folder_path='/home/yzhang39/project-kirby/data/raw/',\n",
    "    processed_folder_path=f'/home/yzhang39/project-kirby/data/processed/ibl',\n",
    "    # metadata for the dataset\n",
    "    experiment_name=eid,\n",
    "    origin_version='',\n",
    "    derived_version='',\n",
    "    source='',\n",
    "    description='',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with db.new_session() as session:\n",
    "\n",
    "    # extract subject metadata\n",
    "    # this dataset is from dandi, which has structured subject metadata, so we\n",
    "    # can use the helper function extract_subject_from_nwb\n",
    "    subject = SubjectDescription(\n",
    "        id=meta_data['subject'],\n",
    "        species=Species.from_string('MUS_MUSCULUS'),\n",
    "        sex=Sex.from_string('MALE'),\n",
    "    )\n",
    "    session.register_subject(subject)\n",
    "\n",
    "    # extract experiment metadata\n",
    "    # recording_date = nwbfile.session_start_time.strftime(\"%Y%m%d\")\n",
    "    session_id = eid\n",
    "\n",
    "    # register session\n",
    "    session.register_session(\n",
    "        id=session_id,\n",
    "        recording_date=datetime.today().strftime('%Y%m%d'),\n",
    "        task=Task.FREE_BEHAVIOR,\n",
    "    )\n",
    "\n",
    "    # register sortset\n",
    "    session.register_sortset(\n",
    "        id=session_id,\n",
    "        units=units,\n",
    "    )\n",
    "\n",
    "    # register session\n",
    "    session_start, session_end = (\n",
    "        behavior.timestamps[0].item(),\n",
    "        behavior.timestamps[-1].item(),\n",
    "    )\n",
    "\n",
    "    data = Data(\n",
    "        # neural activity\n",
    "        spikes=spikes,\n",
    "        units=units,\n",
    "        # stimuli and behavior\n",
    "        trials=trials,\n",
    "        behavior=behavior,\n",
    "        # domain\n",
    "        domain=Interval(session_start, session_end),\n",
    "    )\n",
    "\n",
    "    session.register_data(data)\n",
    "\n",
    "    # split and register trials into train, validation and test\n",
    "    train_trials = trials.select_by_mask(trials.train_mask_nwb)\n",
    "    valid_trials = trials.select_by_mask(trials.val_mask_nwb)\n",
    "    test_trials = trials.select_by_mask(trials.test_mask_nwb)\n",
    "\n",
    "    session.register_split(\"train\", train_trials)\n",
    "    session.register_split(\"valid\", valid_trials)\n",
    "    session.register_split(\"test\", test_trials)\n",
    "\n",
    "    # save data to disk\n",
    "    session.save_to_disk()\n",
    "\n",
    "# all sessions added, finish by generating a description file for the entire dataset\n",
    "db.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# EVAL\n",
    "# Need to bin spikes and standardize behavior before evaluation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
