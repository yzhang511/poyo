{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from one.api import ONE\n",
    "path_root = '/home/yzhang39/IBL_foundation_model/'\n",
    "sys.path.append(str(path_root))\n",
    "from src.utils.ibl_data_utils import (\n",
    "    prepare_data,\n",
    "    select_brain_regions, \n",
    "    list_brain_regions, \n",
    "    bin_spiking_data,\n",
    "    load_anytime_behaviors\n",
    ")\n",
    "from kirby.data import Data, IrregularTimeSeries, Interval, DatasetBuilder, ArrayDict\n",
    "\n",
    "from datetime import datetime\n",
    "from kirby.taxonomy import (\n",
    "    Task,\n",
    "    Sex,\n",
    "    Species,\n",
    "    SubjectDescription,\n",
    ")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = ONE(\n",
    "    base_url='https://openalyx.internationalbrainlab.org', \n",
    "    password='international', mode='remote'\n",
    ")\n",
    "\n",
    "freeze_file = f'{path_root}/data/2023_12_bwm_release.csv'\n",
    "bwm_df = pd.read_csv(freeze_file, index_col=0)\n",
    "\n",
    "eid = 'db4df448-e449-4a6f-a0e7-288711e7a75a'\n",
    "\n",
    "params = {\n",
    "    'interval_len': 2, 'binsize': 0.02, 'single_region': False, \n",
    "    'align_time': 'stimOn_times', 'time_window': (-.5, 1.5), 'fr_thresh': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'DY_009',\n",
       " 'lab': 'danlab',\n",
       " 'projects': 'ibl_neuropixel_brainwide_01',\n",
       " 'task_protocol': '_iblrig_tasks_ephysChoiceWorld6.2.5',\n",
       " 'number': 1,\n",
       " 'start_time': '2020-02-27T11:59:23.251760',\n",
       " 'url': 'https://openalyx.internationalbrainlab.org/sessions/db4df448-e449-4a6f-a0e7-288711e7a75a',\n",
       " 'local_path': PosixPath('/home/yzhang39/Downloads/ONE/openalyx.internationalbrainlab.org/danlab/Subjects/DY_009/2020-02-27/001'),\n",
       " 'date': datetime.date(2020, 2, 27)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one.get_details(eid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1 probes for session eid: db4df448-e449-4a6f-a0e7-288711e7a75a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use spikes from brain regions:  ['CA1' 'DG' 'LP' 'PoT' 'SGN' 'SNc' 'SPF' 'VISp' 'ZI' 'root']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402/402 [00:03<00:00, 116.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# CAUTION: Match trial selection criteria\n",
    "neural_dict, _, meta_data, trials_data = prepare_data(one, eid, bwm_df, params, n_workers=1)\n",
    "regions, beryl_reg = list_brain_regions(neural_dict, **params)\n",
    "region_cluster_ids = select_brain_regions(neural_dict, beryl_reg, regions, **params)\n",
    "binned_spikes, clusters_used_in_bins = bin_spiking_data(\n",
    "    region_cluster_ids, neural_dict, trials_df=trials_data['trials_df'], n_workers=1, **params\n",
    ")\n",
    "avg_fr = binned_spikes.sum(1).mean(0) / params['interval_len']\n",
    "active_neuron_ids = np.argwhere(avg_fr > 1/params['fr_thresh']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract spiking activity\n",
    "spike_times = neural_dict['spike_times']\n",
    "spike_clusters = neural_dict['spike_clusters']\n",
    "unit_mask = np.isin(spike_clusters, active_neuron_ids)\n",
    "spike_times = spike_times[unit_mask]\n",
    "spike_clusters = spike_clusters[unit_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_ids = np.array(meta_data['uuids'])[active_neuron_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_meta = []\n",
    "timestamps = []\n",
    "unit_index = []\n",
    "\n",
    "for i in range(len(unit_ids)):\n",
    "\n",
    "    unit_id = unit_ids[i]\n",
    "    \n",
    "    # extract spikes\n",
    "    times = spike_times[spike_clusters == i]\n",
    "    timestamps.append(times)\n",
    "\n",
    "    if len(times) > 0:\n",
    "        unit_index.append([i] * len(times))\n",
    "\n",
    "    # extract unit metadata\n",
    "    unit_meta.append(\n",
    "        {\n",
    "            \"id\": unit_id,\n",
    "            \"unit_number\": i,\n",
    "            \"count\": len(times),\n",
    "            \"type\": 0,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "unit_meta_df = pd.DataFrame(unit_meta)  # list of dicts to dataframe\n",
    "units = ArrayDict.from_dataframe(\n",
    "    unit_meta_df,\n",
    "    unsigned_to_long=True,\n",
    ")\n",
    "\n",
    "# concatenate spikes\n",
    "timestamps = np.concatenate(timestamps)\n",
    "unit_index = np.concatenate(unit_index)\n",
    "\n",
    "# create spikes object\n",
    "spikes = IrregularTimeSeries(\n",
    "    timestamps=timestamps,\n",
    "    unit_index=unit_index,\n",
    "    domain=\"auto\",\n",
    ")\n",
    "\n",
    "# make sure to sort ethe spikes\n",
    "spikes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_trials\n",
    "# CAUTION: Need to exclude trials in which behavior is NONE\n",
    "\n",
    "trial_mask = trials_data['trials_mask']\n",
    "start_time = (trials_data['trials_df'][params['align_time']] - params['time_window'][0])[trial_mask]\n",
    "end_time = (trials_data['trials_df'][params['align_time']] + params['time_window'][1])[trial_mask]\n",
    "\n",
    "max_num_trials = sum(trial_mask)\n",
    "trial_idxs = np.random.choice(np.arange(max_num_trials), max_num_trials, replace=False)\n",
    "train_idxs = trial_idxs[:int(0.7*max_num_trials)]\n",
    "val_idxs = trial_idxs[int(0.7*max_num_trials):int(0.8*max_num_trials)]\n",
    "test_idxs = trial_idxs[int(0.8*max_num_trials):]\n",
    "trial_split = np.array(['train'] * max_num_trials)\n",
    "trial_split[val_idxs] = 'val'\n",
    "trial_split[test_idxs] = 'test'\n",
    "\n",
    "trial_table = pd.DataFrame({\n",
    "    \"start\": start_time,\n",
    "    \"end\": end_time,\n",
    "    \"split_indicator\": trial_split,\n",
    "})\n",
    "trials = Interval.from_dataframe(trial_table)\n",
    "\n",
    "train_mask_nwb = trial_table.split_indicator.to_numpy() == \"train\"\n",
    "val_mask_nwb = trial_table.split_indicator.to_numpy() == \"val\"\n",
    "test_mask_nwb = trial_table.split_indicator.to_numpy() == \"test\"\n",
    "\n",
    "trials.train_mask_nwb = (\n",
    "    train_mask_nwb  # Naming with \"_\" since train_mask is reserved\n",
    ")\n",
    "trials.val_mask_nwb = val_mask_nwb\n",
    "trials.test_mask_nwb = test_mask_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract discrete behaviors\n",
    "def map_choice(data):\n",
    "    choice_map = {\"-1\": 0, \"1\": 1}\n",
    "    return choice_map[str(int(data))]\n",
    "\n",
    "def map_block(data):\n",
    "    block_map = {\"0.2\": 0, \"0.5\": 1, \"0.8\": 2}\n",
    "    return block_map[str(data)]\n",
    "    \n",
    "choice = trials_data['trials_df'].choice[trial_mask]\n",
    "block = trials_data['trials_df'].probabilityLeft[trial_mask]\n",
    "\n",
    "stim_start_times = start_time.to_numpy()\n",
    "stim_end_times = end_time.to_numpy()\n",
    "\n",
    "# create data object for choice and block\n",
    "choice = Interval(\n",
    "    start=stim_start_times,\n",
    "    end=stim_end_times,\n",
    "    choice=choice.apply(map_choice).to_numpy(),\n",
    "    timestamps=stim_start_times / 2.0 + stim_end_times / 2.0,\n",
    "    timekeys=[\"start\", \"end\", \"timestamps\"],\n",
    ")\n",
    "\n",
    "block = Interval(\n",
    "    start=stim_start_times,\n",
    "    end=stim_end_times,\n",
    "    choice=block.apply(map_block).to_numpy(),\n",
    "    timestamps=stim_start_times / 2.0 + stim_end_times / 2.0,\n",
    "    timekeys=[\"start\", \"end\", \"timestamps\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract_behavior\n",
    "behave_dict = load_anytime_behaviors(one, eid, n_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = behave_dict['right-whisker-motion-energy']['times']\n",
    "whisker = behave_dict['right-whisker-motion-energy']['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_type = np.ones_like(timestamps, dtype=np.int64) * 0\n",
    "\n",
    "# report accuracy only on the evaluation intervals\n",
    "eval_mask = np.zeros_like(timestamps, dtype=bool)\n",
    "\n",
    "for i in range(len(trials)):\n",
    "    eval_mask[\n",
    "        (timestamps >= trials.start[i]) & (timestamps < trials.end[i])\n",
    "    ] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = IrregularTimeSeries(\n",
    "    timestamps=timestamps,\n",
    "    whisker=whisker.reshape(-1,1),\n",
    "    subtask_index=behavior_type,\n",
    "    eval_mask=eval_mask,\n",
    "    domain=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(\n",
    "    # neural activity\n",
    "    spikes=spikes,\n",
    "    units=units,\n",
    "    # stimuli and behavior\n",
    "    trials=trials,\n",
    "    behavior=behavior,\n",
    "    # domain\n",
    "    domain=Interval(trials.start[0], trials.end[-1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DatasetBuilder(\n",
    "    raw_folder_path='/home/yzhang39/project-kirby/data/raw/',\n",
    "    processed_folder_path=f'/home/yzhang39/project-kirby/data/processed/ibl',\n",
    "    # metadata for the dataset\n",
    "    experiment_name=eid,\n",
    "    origin_version='',\n",
    "    derived_version='',\n",
    "    source='',\n",
    "    description='',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with db.new_session() as session:\n",
    "\n",
    "    # extract subject metadata\n",
    "    # this dataset is from dandi, which has structured subject metadata, so we\n",
    "    # can use the helper function extract_subject_from_nwb\n",
    "    subject = SubjectDescription(\n",
    "        id=meta_data['subject'],\n",
    "        species=Species.from_string('MUS_MUSCULUS'),\n",
    "        sex=Sex.from_string('MALE'),\n",
    "    )\n",
    "    session.register_subject(subject)\n",
    "\n",
    "    # extract experiment metadata\n",
    "    # recording_date = nwbfile.session_start_time.strftime(\"%Y%m%d\")\n",
    "    session_id = eid\n",
    "\n",
    "    # register session\n",
    "    session.register_session(\n",
    "        id=session_id,\n",
    "        recording_date=datetime.today().strftime('%Y%m%d'),\n",
    "        task=Task.FREE_BEHAVIOR,\n",
    "    )\n",
    "\n",
    "    # register sortset\n",
    "    session.register_sortset(\n",
    "        id=session_id,\n",
    "        units=units,\n",
    "    )\n",
    "\n",
    "    # register session\n",
    "    session_start, session_end = (\n",
    "        behavior.timestamps[0].item(),\n",
    "        behavior.timestamps[-1].item(),\n",
    "    )\n",
    "\n",
    "    data = Data(\n",
    "        # neural activity\n",
    "        spikes=spikes,\n",
    "        units=units,\n",
    "        # stimuli and behavior\n",
    "        trials=trials,\n",
    "        behavior=behavior,\n",
    "        # domain\n",
    "        domain=Interval(session_start, session_end),\n",
    "    )\n",
    "\n",
    "    session.register_data(data)\n",
    "\n",
    "    # split and register trials into train, validation and test\n",
    "    train_trials = trials.select_by_mask(trials.train_mask_nwb)\n",
    "    valid_trials = trials.select_by_mask(trials.val_mask_nwb)\n",
    "    test_trials = trials.select_by_mask(trials.test_mask_nwb)\n",
    "\n",
    "    session.register_split(\"train\", train_trials)\n",
    "    session.register_split(\"valid\", valid_trials)\n",
    "    session.register_split(\"test\", test_trials)\n",
    "\n",
    "    # save data to disk\n",
    "    session.save_to_disk()\n",
    "\n",
    "# all sessions added, finish by generating a description file for the entire dataset\n",
    "db.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
