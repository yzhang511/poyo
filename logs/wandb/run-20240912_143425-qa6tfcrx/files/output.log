Wandb ID: qa6tfcrx
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-09-12 14:34:26,030][__main__][INFO] - Local rank/node rank/world size/num nodes: 0/0/1/1
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                 | Type                   | Params
----------------------------------------------------------------
0 | model                | POYOPlus               | 1.3 M
1 | model.unit_emb       | InfiniteVocabEmbedding | 11.8 K
2 | model.session_emb    | InfiniteVocabEmbedding | 128
3 | model.spike_type_emb | Embedding              | 256
4 | model.task_emb       | Embedding              | 2.0 K
5 | model.latent_emb     | Embedding              | 1.0 K
6 | model.perceiver_io   | PerceiverRotary        | 1.3 M
7 | model.readout        | MultitaskReadout       | 68.6 K
----------------------------------------------------------------
1.3 M     Trainable params
0         Non-trainable params
1.3 M     Total params
5.346     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Training: 0it [00:00, ?it/s][2024-09-12 14:34:37,225][root][INFO] - Memory info:
MemTotal:       394805420 kB
MemFree:        349482240 kB
MemAvailable:   377173428 kB
Buffers:         1774768 kB
Cached:         27111772 kB
SwapCached:            0 kB
Active:         16808128 kB
Inactive:       22836576 kB
Active(anon):        816 kB
Inactive(anon): 11058808 kB
Active(file):   16807312 kB
Inactive(file): 11777768 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:                32 kB
Writeback:             8 kB
AnonPages:      10696704 kB
Mapped:          2579076 kB
Shmem:            301512 kB
KReclaimable:    2547704 kB
Slab:            3770576 kB
SReclaimable:    2547704 kB
SUnreclaim:      1222872 kB
KernelStack:       17072 kB
PageTables:        59588 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    197402708 kB
Committed_AS:   16375848 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      429700 kB
VmallocChunk:          0 kB
Percpu:           157824 kB
HardwareCorrupted:     0 kB
AnonHugePages:   4157440 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB
DirectMap4k:    32051564 kB
DirectMap2M:    363917312 kB
DirectMap1G:     7340032 kB

Epoch 9: 100%|████████████████torch.Size([8719])████████████████| 1/1 [00:00<00:00,  2.07it/s, v_num=fcrx, train_loss=0.00442]
/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19249])) that is different to the input size (torch.Size([19249, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/utils/train_wrapper.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1760.)
  self.log(f"weights/std_{tag}", value.cpu().std(), sync_dist=True)
/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('epoch_time', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19250])) that is different to the input size (torch.Size([19250, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19256])) that is different to the input size (torch.Size([19256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19244])) that is different to the input size (torch.Size([19244, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19245])) that is different to the input size (torch.Size([19245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19246])) that is different to the input size (torch.Size([19246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19242])) that is different to the input size (torch.Size([19242, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
torch.Size([8719])0:00, ?it/s]
                                                                                                                              /home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([8719])) that is different to the input size (torch.Size([8719, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
val @ Epoch 9: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.67it/s]
Compiling metrics @ Epoch 9:   0%|                                                                      | 0/1 [00:00<?, ?it/s]
Error executing job with overrides: []██████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.18it/s]
Traceback (most recent call last):                                                                      | 0/1 [00:00<?, ?it/s]
  File "train.py", line 276, in main
    run_training(cfg)
  File "train.py", line 261, in run_training
    trainer.fit(
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 532, in fit
    call._call_and_handle_interrupt(
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 571, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 980, in _run
    results = self._run_stage()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
    self.fit_loop.run()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py", line 202, in run
    self.advance()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py", line 355, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 134, in run
    self.on_advance_end()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 249, in on_advance_end
    self.val_loop.run()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py", line 181, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 102, in run
    self.on_run_start()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 235, in on_run_start
    self._on_evaluation_epoch_start()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 317, in _on_evaluation_epoch_start
    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 195, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/home/yzhang39/project-kirby/kirby/utils/validation_wrapper.py", line 294, in on_validation_epoch_start
    return self.run(trainer, pl_module)
  File "/home/yzhang39/project-kirby/kirby/utils/validation_wrapper.py", line 248, in run
    gt = avg_pool(timestamps, gt)
  File "/home/yzhang39/project-kirby/kirby/utils/validation_wrapper.py", line 356, in avg_pool
    pooled_sum.scatter_add_(0, indices_expanded, values)
UnboundLocalError: local variable 'indices_expanded' referenced before assignment

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
