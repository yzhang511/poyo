Wandb ID: vnvp2947
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-09-12 14:26:00,214][__main__][INFO] - Local rank/node rank/world size/num nodes: 0/0/1/1
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                 | Type                   | Params
----------------------------------------------------------------
0 | model                | POYOPlus               | 1.3 M
1 | model.unit_emb       | InfiniteVocabEmbedding | 11.8 K
2 | model.session_emb    | InfiniteVocabEmbedding | 128
3 | model.spike_type_emb | Embedding              | 256
4 | model.task_emb       | Embedding              | 2.0 K
5 | model.latent_emb     | Embedding              | 1.0 K
6 | model.perceiver_io   | PerceiverRotary        | 1.3 M
7 | model.readout        | MultitaskReadout       | 68.6 K
----------------------------------------------------------------
1.3 M     Trainable params
0         Non-trainable params
1.3 M     Total params
5.346     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Training: 0it [00:00, ?it/s][2024-09-12 14:26:17,195][root][INFO] - Memory info:
MemTotal:       394805420 kB
MemFree:        354322576 kB
MemAvailable:   381626932 kB
Buffers:         1774548 kB
Cached:         26694748 kB
SwapCached:            0 kB
Active:         16627788 kB
Inactive:       18318988 kB
Active(anon):        740 kB
Inactive(anon):  6745868 kB
Active(file):   16627048 kB
Inactive(file): 11573120 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:               648 kB
Writeback:           420 kB
AnonPages:       6424264 kB
Mapped:          1674324 kB
Shmem:            269116 kB
KReclaimable:    2545784 kB
Slab:            3755368 kB
SReclaimable:    2545784 kB
SUnreclaim:      1209584 kB
KernelStack:       16880 kB
PageTables:        44632 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    197402708 kB
Committed_AS:   11825624 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      423220 kB
VmallocChunk:          0 kB
Percpu:           157824 kB
HardwareCorrupted:     0 kB
AnonHugePages:   2439168 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB
DirectMap4k:    32049516 kB
DirectMap2M:    363919360 kB
DirectMap1G:     7340032 kB

Epoch 9: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.17it/s, v_num=2947, train_loss=0.00464]
/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19249])) that is different to the input size (torch.Size([19249, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/utils/train_wrapper.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1760.)
  self.log(f"weights/std_{tag}", value.cpu().std(), sync_dist=True)
/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('epoch_time', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19250])) that is different to the input size (torch.Size([19250, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19256])) that is different to the input size (torch.Size([19256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19244])) that is different to the input size (torch.Size([19244, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19245])) that is different to the input size (torch.Size([19245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19246])) that is different to the input size (torch.Size([19246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([19242])) that is different to the input size (torch.Size([19242, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
Validation: 0it [00:00, ?it/s]
                                                                                                                                         /home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/yzhang39/project-kirby/kirby/nn/loss.py:33: UserWarning: Using a target size (torch.Size([8719])) that is different to the input size (torch.Size([8719, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_noreduce = F.mse_loss(output, target, reduction="none").mean(dim=1)
val @ Epoch 9: 100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.67it/s]
Compiling metrics @ Epoch 9:   0%|                                                                                 | 0/1 [00:00<?, ?it/s]
Error executing job with overrides: []█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.21it/s]
Traceback (most recent call last):                                                                                 | 0/1 [00:00<?, ?it/s]
  File "train.py", line 276, in main
    run_training(cfg)
  File "train.py", line 261, in run_training
    trainer.fit(
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 532, in fit
    call._call_and_handle_interrupt(
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 571, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 980, in _run
    results = self._run_stage()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
    self.fit_loop.run()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py", line 202, in run
    self.advance()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py", line 355, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 134, in run
    self.on_advance_end()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 249, in on_advance_end
    self.val_loop.run()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py", line 181, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 102, in run
    self.on_run_start()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 235, in on_run_start
    self._on_evaluation_epoch_start()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 317, in _on_evaluation_epoch_start
    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 195, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/home/yzhang39/project-kirby/kirby/utils/validation_wrapper.py", line 294, in on_validation_epoch_start
    return self.run(trainer, pl_module)
  File "/home/yzhang39/project-kirby/kirby/utils/validation_wrapper.py", line 248, in run
    gt = avg_pool(timestamps, gt)
  File "/home/yzhang39/project-kirby/kirby/utils/validation_wrapper.py", line 351, in avg_pool
    indices_expanded = indices.unsqueeze(-1).expand_as(values)
RuntimeError: expand(torch.LongTensor{[8719, 1]}, size=[8719]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
