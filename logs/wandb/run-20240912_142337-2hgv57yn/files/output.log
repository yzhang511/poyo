Wandb ID: 2hgv57yn
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-09-12 14:23:38,291][__main__][INFO] - Local rank/node rank/world size/num nodes: 0/0/1/1
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                 | Type                   | Params
----------------------------------------------------------------
0 | model                | POYOPlus               | 1.3 M
1 | model.unit_emb       | InfiniteVocabEmbedding | 11.8 K
2 | model.session_emb    | InfiniteVocabEmbedding | 128
3 | model.spike_type_emb | Embedding              | 256
4 | model.task_emb       | Embedding              | 2.0 K
5 | model.latent_emb     | Embedding              | 1.0 K
6 | model.perceiver_io   | PerceiverRotary        | 1.3 M
7 | model.readout        | MultitaskReadout       | 68.6 K
----------------------------------------------------------------
1.3 M     Trainable params
0         Non-trainable params
1.3 M     Total params
5.346     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Training: 0it [00:00, ?it/s][2024-09-12 14:23:49,322][root][INFO] - Memory info:
MemTotal:       394805420 kB
MemFree:        350861000 kB
MemAvailable:   378134580 kB
Buffers:         1774456 kB
Cached:         26672980 kB
SwapCached:            0 kB
Active:         16606716 kB
Inactive:       21666692 kB
Active(anon):        784 kB
Inactive(anon): 10103252 kB
Active(file):   16605932 kB
Inactive(file): 11563440 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:               608 kB
Writeback:           404 kB
AnonPages:       9773408 kB
Mapped:          2515172 kB
Shmem:            278048 kB
KReclaimable:    2545804 kB
Slab:            3764336 kB
SReclaimable:    2545804 kB
SUnreclaim:      1218532 kB
KernelStack:       16896 kB
PageTables:        56972 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    197402708 kB
Committed_AS:   15220908 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      429636 kB
VmallocChunk:          0 kB
Percpu:           157824 kB
HardwareCorrupted:     0 kB
AnonHugePages:   3418112 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB
DirectMap4k:    32049516 kB
DirectMap2M:    363919360 kB
DirectMap1G:     7340032 kB

Epoch 0:   0%|                                                                                                   | 0/1 [00:00<?, ?it/s]
/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Error executing job with overrides: []
Traceback (most recent call last):
  File "train.py", line 276, in main
    run_training(cfg)
  File "train.py", line 261, in run_training
    trainer.fit(
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 532, in fit
    call._call_and_handle_interrupt(
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 571, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 980, in _run
    results = self._run_stage()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
    self.fit_loop.run()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py", line 202, in run
    self.advance()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py", line 355, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 190, in advance
    batch = next(data_fetcher)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/fetchers.py", line 126, in __next__
    batch = super().__next__()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/loops/fetchers.py", line 58, in __next__
    batch = next(iterator)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/utilities/combined_loader.py", line 285, in __next__
    out = next(self._iterator)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/lightning/pytorch/utilities/combined_loader.py", line 65, in __next__
    out[i] = next(self.iterators[i])
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
    return self._process_data(data)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
    data.reraise()
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/_utils.py", line 722, in reraise
    raise exception
AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/yzhang39/project-kirby/kirby/data/data.py", line 2937, in get_nested_attribute
    out = getattr(out, c)
AttributeError: 'Data' object has no attribute 'running_speed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/yzhang39/miniconda3/envs/poyo/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/yzhang39/project-kirby/kirby/data/dataset.py", line 428, in __getitem__
    sample = self.transform(sample)
  File "/home/yzhang39/project-kirby/kirby/transforms/container.py", line 22, in __call__
    data = transform(data)
  File "/home/yzhang39/project-kirby/kirby/models/poyo_plus.py", line 263, in __call__
    ) = prepare_for_multitask_readout(
  File "/home/yzhang39/project-kirby/kirby/nn/multitask_readout.py", line 166, in prepare_for_multitask_readout
    values[key] = data.get_nested_attribute(decoder["value_key"])
  File "/home/yzhang39/project-kirby/kirby/data/data.py", line 2939, in get_nested_attribute
    raise AttributeError(
AttributeError: Could not resolve running_speed.running_speed in data (specifically, at level running_speed))


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
