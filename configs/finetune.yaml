# Path: configs/train.yaml
defaults:
  - _self_
  - model: poyo_single_session.yaml
  - train_datasets: perich_single_session.yaml
  - val_datasets: perich_single_session.yaml

train_transforms:
  - _target_: kirby.transforms.UnitDropout
    max_units: 1000
    min_units: 60
    mode_units: 300
    peak: 4
  - _target_: kirby.transforms.RandomCrop
    crop_len: 1.0

data_root: ./data/processed/
seed: 42
batch_size: 128
eval_epochs: 5
epochs: 100
steps: 0  # Note we either specify epochs or steps, not both.
base_lr: 7.8125e-7
weight_decay: 0.0001
# Fraction of epochs to warmup for.
pct_start: 0.5
num_workers: 4
log_dir: ./logs
name: perich_finetune
precision: 32
nodes: 1
gpus: 1
# Where to resume/finetune from. Could be null (yaml for None, meaning train from
# scratch) or a fully qualified path to the .ckpt file.
ckpt_path: logs/lightning_logs/f9sj5g0b/last.ckpt

# Finetuning configuration:
finetune: true
# Num of epochs to freeze perceiver network while finetuning
# -1 => Keep frozen, i.e. perform Unit-identification
#  0 => Train everything
# >0 => Only train unit/session embeddings for first few epochs,
#       and then train everything
freeze_perceiver_until_epoch: 10
